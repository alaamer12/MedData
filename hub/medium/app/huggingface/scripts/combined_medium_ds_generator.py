# -*- coding: utf-8 -*-
"""draft_for_post_generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w88gzvdcNxg1zRlN5OqEPqLhYhTEARnn
"""
import os
TOKEN = os.environ["HF_TOKEN"]

!pip install datasets

!pip install kagglehub huggingface_hub tqdm

from huggingface_hub import login, whoami
login(TOKEN, True)
whoami()

KAGGLE_DATASETS = {
            "aiswaryaramachandran/medium-articles-with-content": "/content/1/medium-articles-with-content/2/Medium_AggregatedData.csv",
            "hsankesara/medium-articles": "/content/2/medium-articles/1/articles.csv",
            "meruvulikith/1300-towards-datascience-medium-articles-dataset": "/content/3/1300-towards-datascience-medium-articles-dataset/1/medium.csv"
}

HUGGINGFACE_DATASET = [
    "fabiochiu/medium-articles",
    # Requires Hugging face auth
    "Falah/medium_articles_posts"]

MY_DATASET_NAME = "Alaamer/medium-articles-posts-with-content"

# Save locally as Parquet
PARQUET_PATH = "large_dataset.parquet"

import kagglehub
import shutil
import os

def move_ds(dataset_path, des):
    """Moves the downloaded dataset to the specified destination folder.

    Args:
        dataset_path (str): The path to the downloaded dataset.
        des (str): The destination folder.
    """
    # Create the destination directory if it doesn't exist
    os.makedirs(des, exist_ok=True)
    # Move the dataset to the destination
    shutil.move(dataset_path, des)

def _delete_on_exist(dataset_path):
    """Deletes the downloaded dataset if it already exists in the destination folder.

    Args:
        dataset_path (str): The path to the downloaded dataset.
    """
    # Check if the dataset already exists in the destination folder
    if os.path.exists(dataset_path):
        # Delete the dataset
        # Use shutil.rmtree to delete a directory and its contents
        if os.path.isdir(dataset_path):
            shutil.rmtree(dataset_path)
        else:  # If it's a file, use os.remove
            os.remove(dataset_path)

def download_kaggle_ds(dataset, des, delete_on_exist=False):
    """Downloads a Kaggle dataset and moves it to the specified destination.

    Args:
        dataset (str): The name of the Kaggle dataset.
        des (str): The destination folder.
    """
    # Download the latest version of the dataset
    # Pass the specific file path to _delete_on_exist,
    # not the parent directory
    if delete_on_exist:
        _delete_on_exist(os.path.join(des, dataset.split("/")[-1]))
    dataset_path = kagglehub.dataset_download(dataset)
    print("Path to dataset files:", dataset_path)
    # Extract the dataset name from the path
    dataset_name = dataset.split("/")[-1]
    # Move the downloaded dataset to the destination folder
    move_ds(dataset_path, os.path.join(des, dataset_name))

def get_file_size_mb(file_path):
  """Gets the size of a file in megabytes (MB).

  Args:
    file_path: The path to the file.

  Returns:
    The size of the file in megabytes, or -1 if the file does not exist.
  """
  try:
    size_bytes = os.path.getsize(file_path)
    size_mb = size_bytes / (1024 * 1024)  # Convert bytes to MB
    return size_mb
  except FileNotFoundError:
    return -1

file_path = '/content/large_dataset.parquet'
file_size_mb = get_file_size_mb(file_path)

if file_size_mb != -1:
  print(f"The size of {file_path} is {file_size_mb:.2f} MB.")  # Format to 2 decimal places
else:
  print(f"File not found: {file_path}")

from datasets import load_dataset
import pandas as pd

def normalized_df(df):
   # Print the shape before deleting rows
    print("Shape before:", df.shape)

    # Handle both 'text' and 'Text' column names
    text_col = None
    if 'text' in df.columns:
        text_col = 'text'
    elif 'Text' in df.columns:
        text_col = 'Text'

    if text_col:
        # Drop rows with null text values
        df = df[df[text_col].notna()]

        # Drop duplicate rows based on the text column, keeping the first occurrence
        df.drop_duplicates(subset=[text_col], keep='first', inplace=True)

    # Print the shape after deleting rows
    print("Shape after:", df.shape)
    return df

def read_kaggle_and_normalize_df(file_path):
    # Read the CSV file into a pandas DataFrame
    df = pd.read_csv(file_path, on_bad_lines='skip')
    return normalized_df(df)

def load_huggigface_and_normalize_ds(d):
  # Load the dataset
  dataset = load_dataset(d)

  # Convert the dataset to a pandas DataFrame
  df = dataset["train"].to_pandas()
  return normalized_df(df)

# Set the maximum column width to display the full text
pd.set_option('display.max_colwidth', None)

from pandas import DataFrame
from tqdm.auto import tqdm

def rich_kaggle_df():
  combined_kaggle_df = DataFrame()

  for i, (d, out_path) in enumerate(tqdm(KAGGLE_DATASETS.items(), desc="Processing Kaggle Datasets")):
    print(f"Downloading {d}")
    download_kaggle_ds(d,f"/content/{i + 1}", True)
    df = read_kaggle_and_normalize_df(out_path)
    combined_kaggle_df = pd.concat([combined_kaggle_df, df], ignore_index=True) if combined_kaggle_df is not None else df
  print("Combined Huggingface dataset Shape :", combined_kaggle_df.shape)
  return combined_kaggle_df


def rich_huggingface_df():
  combined_huggingface_df = DataFrame()

  for d in tqdm(HUGGINGFACE_DATASET, desc="Processing Huggingface data"):
    print(f"Downloading {d}")
    df = load_huggigface_and_normalize_ds(d)
    combined_huggingface_df = pd.concat([combined_huggingface_df, df], ignore_index=True) if combined_huggingface_df is not None else df
  print("Combined Kaggle dataset Shape :", combined_huggingface_df.shape)
  return combined_huggingface_df

def get_full_dataset():
  # Concatenate the DataFrames vertically
  df1 = rich_kaggle_df()
  df2 = rich_huggingface_df()
  combined_df = pd.concat([df1, df2], ignore_index=True)

  return combined_df

combined_df = get_full_dataset()

print("Combined datasets Shape :", combined_df.shape)

combined_df.to_parquet(PARQUET_PATH)

"""# Note

**Don't** forget to create new dataset at huggingface hub otherwise this cell will always fail, also you will need a ***`WRITE`*** TOKEN at from your settings
"""

# Upload using the `datasets` library
from datasets import load_dataset

dataset = load_dataset("parquet", data_files=PARQUET_PATH)
dataset.push_to_hub(MY_DATASET_NAME)

from datasets import load_dataset

def load_my_ds():
  # Load the dataset
  dataset = load_dataset(MY_DATASET_NAME)

  # Convert to a Pandas DataFrame (if needed)
  df = dataset['train'].to_pandas()
  return df