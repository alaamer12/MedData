# www.robotstxt.org/
# Optimize crawling for search engines

User-agent: *
Allow: /
Allow: /common/
Allow: /medium-dataset/
Allow: /devto-dataset/
Allow: /docs/
Allow: /assets/images/

# Temporary files
Disallow: /temp/
Disallow: /.git/
Disallow: /.github/
Disallow: /node_modules/
Disallow: /__pycache__/

# API endpoints (if any in the future)
Disallow: /api/

# Prevent duplicate content
Disallow: /*?*sort=
Disallow: /*?*filter=
Disallow: /*?*page=

# Sitemaps
Sitemap: https://meddata.ai/sitemap.xml

# Crawl-delay for specific bots
User-agent: AhrefsBot
Crawl-delay: 10

User-agent: Baiduspider
Crawl-delay: 10

User-agent: Yandex
Crawl-delay: 10

# Allow Google to crawl JavaScript and CSS
User-agent: Googlebot
Allow: *.js
Allow: *.css
Allow: *.svg
Allow: *.png
Allow: *.jpg
Allow: *.gif

# Allow Bing to crawl JavaScript and CSS
User-agent: Bingbot
Allow: *.js
Allow: *.css
Allow: *.svg
Allow: *.png
Allow: *.jpg
Allow: *.gif
